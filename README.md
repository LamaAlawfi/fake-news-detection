# 📰 Fake News Detection with Fine-Tuned Transformers  

This project compares a fine-tuned transformer model (**DistilBERT**) with a classical baseline (**TF–IDF + Naïve Bayes**) for detecting fake news.  
It demonstrates how contextual embeddings from transformers significantly outperform feature-based approaches in natural language processing (NLP).  

---

## 📂 Project Structure
- **notebooks/fake_news_detection.ipynb** → Jupyter Notebook with code, training, and evaluation.  
- **reports/project-report.pdf** → Full project report with literature review, methodology, and results.  
- **README.md** → Documentation (this file).  
- **.gitignore** → Python ignores.  
- **LICENSE** → MIT license.  

---

## 🚀 Models
- **DistilBERT (fine-tuned)** → achieved ~100% accuracy on test data.  
- **TF–IDF + Naïve Bayes** → strong baseline with ~93% accuracy.  

---

## 📊 Results
| Model                  | Accuracy | Precision | Recall | F1-score | MCC  |
|-------------------------|----------|-----------|--------|----------|------|
| DistilBERT (Fine-tuned) | 100%     | 0.92      | 0.94   | 0.93     | 0.86 |
| TF–IDF + Naïve Bayes    | 93%      | 0.85      | 0.75   | 0.80     | 0.60 |

---

## ⚙️ Technologies
- Python 3.9+  
- HuggingFace Transformers  
- PyTorch  
- Scikit-learn  
- Pandas, NumPy, Matplotlib  

---

## 🔮 Future Work
- Zero-shot and few-shot fake news detection using large language models (LLMs).  
- Multilingual detection (mBERT, XLM-R).  
- Handling satire and irony in classification.  
- Human-in-the-loop fact-checking for reliability.  

---

## 📄 Report
The full article with methodology, literature review, and detailed results is available here:  
[📕 Project Report](./reports/project-report.pdf)  

---

## 👥 Team
- Roaa Al-Mdani  
- Lama Al-Alawfi  
- Shatha Mahrous  

**Supervised by:** Dr. Ahmed Elhyek  

---

## 📄 License
This project is licensed under the MIT License.  
# 📰 Fake News Detection with Fine-Tuned Transformers  

This project compares a fine-tuned transformer model (**DistilBERT**) with a classical baseline (**TF–IDF + Naïve Bayes**) for detecting fake news.  
It demonstrates how contextual embeddings from transformers significantly outperform feature-based approaches in natural language processing (NLP).  

---

## 📂 Project Structure
- **notebooks/fake_news_detection.ipynb** → Jupyter Notebook with code, training, and evaluation.  
- **reports/project-report.pdf** → Full project report with literature review, methodology, and results.  
- **README.md** → Documentation (this file).  
- **.gitignore** → Python ignores.  
- **LICENSE** → MIT license.  

---

## 🚀 Models
- **DistilBERT (fine-tuned)** → achieved ~100% accuracy on test data.  
- **TF–IDF + Naïve Bayes** → strong baseline with ~93% accuracy.  

---

## 📊 Results
| Model                  | Accuracy | Precision | Recall | F1-score | MCC  |
|-------------------------|----------|-----------|--------|----------|------|
| DistilBERT (Fine-tuned) | 100%     | 0.92      | 0.94   | 0.93     | 0.86 |
| TF–IDF + Naïve Bayes    | 93%      | 0.85      | 0.75   | 0.80     | 0.60 |

---

## ⚙️ Technologies
- Python 3.9+  
- HuggingFace Transformers  
- PyTorch  
- Scikit-learn  
- Pandas, NumPy, Matplotlib  

---

## 🔮 Future Work
- Zero-shot and few-shot fake news detection using large language models (LLMs).  
- Multilingual detection (mBERT, XLM-R).  
- Handling satire and irony in classification.  
- Human-in-the-loop fact-checking for reliability.  

---

## 📄 Report
The full article with methodology, literature review, and detailed results is available here:  
[📕 Project Report](./reports/project-report.pdf)  

---

## 👥 Team
- Roaa Al-Mdani  
- Lama Al-Alawfi  
- Shatha Mahrous  


---

## 📄 License
This project is licensed under the MIT License.  
icense
This project is licensed under the MIT License.

