# ğŸ“° Fake News Detection  

This project compares a **fine-tuned transformer model (DistilBERT)** against a **traditional TFâ€“IDF + NaÃ¯ve Bayes baseline** for detecting fake news. It demonstrates how contextual embeddings from transformers outperform classical feature-based methods in text classification.  

---

## ğŸ“‚ Project Structure
- **fake_news_detection.ipynb** â†’ Jupyter Notebook with experiments, preprocessing, and training.  
- **README.md** â†’ Documentation for the project.  
- **.gitignore** â†’ Ignores unnecessary files (cache, virtual environments, etc.).  
- **LICENSE** â†’ MIT license.  

*(You can also add your PDF report later inside a `reports/` folder for completeness.)*  

---

## ğŸš€ Models
- **DistilBERT (fine-tuned)** â†’ lightweight transformer, achieves ~100% accuracy on the test set.  
- **TFâ€“IDF + NaÃ¯ve Bayes** â†’ strong classical baseline, ~93% accuracy.  

---

## ğŸ“Š Results
| Model                  | Accuracy | Precision | Recall | F1-score | MCC  |
|-------------------------|----------|-----------|--------|----------|------|
| DistilBERT (Fine-tuned) | 100%     | 0.92      | 0.94   | 0.93     | 0.86 |
| TFâ€“IDF + NaÃ¯ve Bayes    | 93%      | 0.85      | 0.75   | 0.80     | 0.60 |

---

## âš™ï¸ Technologies
- Python 3.9+  
- HuggingFace Transformers  
- PyTorch  
- Scikit-learn  
- Pandas, NumPy, Matplotlib  

---

## ğŸ”® Future Work
- Zero-shot and few-shot fake news detection with large language models (LLMs).  
- Multilingual detection (e.g., mBERT, XLM-R).  
- Handling satire/irony detection.  
- Hybrid systems with human-in-the-loop fact-checking.  

---

## ğŸ‘¥ Team
- Roaa Al-Mdani  
- Lama Al-Alawfi  
- Shatha Mahrous  


---

## ğŸ“„ License
This project is licensed under the MIT License.

