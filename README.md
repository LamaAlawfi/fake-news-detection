# ğŸ“° Fake News Detection with Fine-Tuned Transformers  

This project compares a fine-tuned transformer model (**DistilBERT**) with a classical baseline (**TFâ€“IDF + NaÃ¯ve Bayes**) for detecting fake news.  
It demonstrates how contextual embeddings from transformers significantly outperform feature-based approaches in natural language processing (NLP).  

---

## ğŸ“‚ Project Structure
- **notebooks/fake_news_detection.ipynb** â†’ Jupyter Notebook with code, training, and evaluation.  
- **reports/project-report.pdf** â†’ Full project report with literature review, methodology, and results.  
- **README.md** â†’ Documentation (this file).  
- **.gitignore** â†’ Python ignores.  
- **LICENSE** â†’ MIT license.  

---

## ğŸš€ Models
- **DistilBERT (fine-tuned)** â†’ achieved ~100% accuracy on test data.  
- **TFâ€“IDF + NaÃ¯ve Bayes** â†’ strong baseline with ~93% accuracy.  

---

## ğŸ“Š Results
| Model                  | Accuracy | Precision | Recall | F1-score | MCC  |
|-------------------------|----------|-----------|--------|----------|------|
| DistilBERT (Fine-tuned) | 100%     | 0.92      | 0.94   | 0.93     | 0.86 |
| TFâ€“IDF + NaÃ¯ve Bayes    | 93%      | 0.85      | 0.75   | 0.80     | 0.60 |

---

## âš™ï¸ Technologies
- Python 3.9+  
- HuggingFace Transformers  
- PyTorch  
- Scikit-learn  
- Pandas, NumPy, Matplotlib  

---

## ğŸ”® Future Work
- Zero-shot and few-shot fake news detection using large language models (LLMs).  
- Multilingual detection (mBERT, XLM-R).  
- Handling satire and irony in classification.  
- Human-in-the-loop fact-checking for reliability.  

---

## ğŸ“„ Report
The full article with methodology, literature review, and detailed results is available here:  
[ğŸ“• Project Report](./reports/project-report.pdf)  

---

## ğŸ‘¥ Team
- Roaa Al-Mdani  
- Lama Al-Alawfi  
- Shatha Mahrous  

**Supervised by:** Dr. Ahmed Elhyek  

---

## ğŸ“„ License
This project is licensed under the MIT License.  
# ğŸ“° Fake News Detection with Fine-Tuned Transformers  

This project compares a fine-tuned transformer model (**DistilBERT**) with a classical baseline (**TFâ€“IDF + NaÃ¯ve Bayes**) for detecting fake news.  
It demonstrates how contextual embeddings from transformers significantly outperform feature-based approaches in natural language processing (NLP).  

---

## ğŸ“‚ Project Structure
- **notebooks/fake_news_detection.ipynb** â†’ Jupyter Notebook with code, training, and evaluation.  
- **reports/project-report.pdf** â†’ Full project report with literature review, methodology, and results.  
- **README.md** â†’ Documentation (this file).  
- **.gitignore** â†’ Python ignores.  
- **LICENSE** â†’ MIT license.  

---

## ğŸš€ Models
- **DistilBERT (fine-tuned)** â†’ achieved ~100% accuracy on test data.  
- **TFâ€“IDF + NaÃ¯ve Bayes** â†’ strong baseline with ~93% accuracy.  

---

## ğŸ“Š Results
| Model                  | Accuracy | Precision | Recall | F1-score | MCC  |
|-------------------------|----------|-----------|--------|----------|------|
| DistilBERT (Fine-tuned) | 100%     | 0.92      | 0.94   | 0.93     | 0.86 |
| TFâ€“IDF + NaÃ¯ve Bayes    | 93%      | 0.85      | 0.75   | 0.80     | 0.60 |

---

## âš™ï¸ Technologies
- Python 3.9+  
- HuggingFace Transformers  
- PyTorch  
- Scikit-learn  
- Pandas, NumPy, Matplotlib  

---

## ğŸ”® Future Work
- Zero-shot and few-shot fake news detection using large language models (LLMs).  
- Multilingual detection (mBERT, XLM-R).  
- Handling satire and irony in classification.  
- Human-in-the-loop fact-checking for reliability.  

---

## ğŸ“„ Report
The full article with methodology, literature review, and detailed results is available here:  
[ğŸ“• Project Report](./reports/project-report.pdf)  

---

## ğŸ‘¥ Team
- Roaa Al-Mdani  
- Lama Al-Alawfi  
- Shatha Mahrous  


---

## ğŸ“„ License
This project is licensed under the MIT License.  
icense
This project is licensed under the MIT License.

